# Best-Link-For-Best-Topics
## Only one best link for best topic
This is a simple link sharing `readme` for best short topics on internet. 
You can contribute. Please send me a pull request. 



# Machine Learning
- [Activation Functions](https://www.youtube.com/watch?v=9vB5nzrL4hY)
- [RRN, LSTM Intro](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  - [LSTM More](https://towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47)
- [Attention](https://www.youtube.com/watch?v=W2rWgXJBZhU&pbjreload=10)
  - [Attention Article](https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129)
- [Flattening](https://www.superdatascience.com/convolutional-neural-networks-cnn-step-3-flattening/)
- [Dropout](https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5)
- [Gradient](https://youtu.be/tIpKfDc295M)
- [Pooling](http://deeplearning.stanford.edu/tutorial/supervised/Pooling/)
- [Probability Distribution](https://youtu.be/cqK3uRoPtk0?t=1)
- [LSTM vs Bi-LSTM](https://stackoverflow.com/questions/43035827/whats-the-difference-between-a-bidirectional-lstm-and-an-lstm)
- [seq2seq LSTM](https://towardsdatascience.com/seq2seq-model-in-tensorflow-ec0c557e560f)
  - [seq2seq LSTM](https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/)
  - [seq2seq Keras model for Translation](https://nextjournal.com/gkoehler/machine-translation-seq2seq-cpu)
- [Tensorflow Basics](https://www.easy-tensorflow.com/tf-tutorials/basics)
- [Policy Gradient](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)
- [Accuracy, Precision, Recall & F1 Score](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/)
- [Optimization Algorithm](https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f)
# Python
- [Built-in Exceptions](https://docs.python.org/3/library/exceptions.html#concrete-exceptions)
  - [Example and Explanation](https://realpython.com/python-exceptions/)
- [Python Numpy Tutorial](http://cs231n.github.io/python-numpy-tutorial/)
- [ _ ](https://hackernoon.com/understanding-the-underscore-of-python-309d1a029edc)
- [ **, ^, %, // ](https://stackoverflow.com/questions/15193927/what-do-these-operators-mean)
- [Lambda](https://www.w3schools.com/python/python_lambda.asp)
- [\__init\__ & self](https://micropyramid.com/blog/understand-self-and-__init__-method-in-python-class/)


# Papers
## Machine Learning
  - [Attention](https://nlp.stanford.edu/pubs/emnlp15_attn.pdf)
  - [LSTM](https://www.bioinf.jku.at/publications/older/2604.pdf)
  - [Overview of Gradient Descent Optimization Algorithm](https://arxiv.org/pdf/1609.04747.pdf)
  
# State of Art Deep Learning
  - [Question Answer](https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art))
  - [Paraphrasing](https://aclweb.org/aclwiki/Paraphrase_Identification_(State_of_the_art))
  - [POS Tagging](https://aclweb.org/aclwiki/POS_Tagging_(State_of_the_art))
  


